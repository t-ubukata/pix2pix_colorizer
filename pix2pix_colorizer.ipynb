{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1utbPj8FM8tLLnTlFdec4KtREYrMbU7gI","timestamp":1547441579580},{"file_id":"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb","timestamp":1546094799988},{"file_id":"1eb0NOTQapkYs3X0v-zL1x5_LFKgDISnp","timestamp":1527173385672}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0TD5ZrvEMbhZ"},"source":["##### Copyright 2018 The TensorFlow Authors.\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\").\n","\n","# Pix2Pix Cololizer: An example with tf.keras and eager\n","\n","<table class=\"tfo-notebook-buttons\" align=\"left\"><td>\n","<a target=\"_blank\"  href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb\">\n","    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>  \n","</td><td>"]},{"cell_type":"markdown","metadata":{"id":"e1_Y75QXJS6h"},"source":["## Import TensorFlow and enable eager execution"]},{"cell_type":"code","metadata":{"id":"YfIk2es3hJEd"},"source":["# Import TensorFlow >= 1.10 and enable eager execution\n","import tensorflow as tf\n","tf.enable_eager_execution()\n","\n","import os\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import PIL\n","from google.colab import files\n","from IPython.display import clear_output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iYn4MdZnKCey"},"source":["## Load the dataset\n","\n","You can download this dataset and similar datasets from [here](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets). As mentioned in the [paper](https://arxiv.org/abs/1611.07004) we apply random jittering and mirroring to the training dataset.\n","* In random jittering, the image is resized to `286 x 286` and then randomly cropped to `256 x 256`\n","* In random mirroring, the image is randomly flipped horizontally i.e left to right."]},{"cell_type":"code","metadata":{"id":"Kn-k8kTXuAlv","outputId":"1e681027-9116-480e-8165-2d21d91d187a","executionInfo":{"status":"ok","timestamp":1547948541501,"user_tz":-540,"elapsed":159531,"user":{"displayName":"j s","photoUrl":"","userId":"00392485866776949616"}},"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":2571}},"source":["print(\"Upload tagged-anime-illustrations-141-real-edge-pair.tar.xz\")\n","files.upload()\n","!tar xvf tagged-anime-illustrations-141-real-edge-pair.tar.xz\n","PATH = 'tagged-anime-illustrations-141-real-edge-pair/'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Upload tagged-anime-illustrations-141-real-edge-pair.tar.xz\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-0e692874-96de-4d81-aa6c-f3b5c3398078\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-0e692874-96de-4d81-aa6c-f3b5c3398078\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving tagged-anime-illustrations-141-real-edge-pair.tar.xz to tagged-anime-illustrations-141-real-edge-pair.tar.xz\n","tagged-anime-illustrations-141-real-edge-pair/\n","tagged-anime-illustrations-141-real-edge-pair/._.DS_Store\n","tagged-anime-illustrations-141-real-edge-pair/.DS_Store\n","tagged-anime-illustrations-141-real-edge-pair/test/\n","tagged-anime-illustrations-141-real-edge-pair/train/\n","tagged-anime-illustrations-141-real-edge-pair/train/1092059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1080059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1057059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1034059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1134059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1045059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/113059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1026059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1073059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1061059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1002059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1102059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1059059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1124059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1024059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1047059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1036059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1000059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1100059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1063059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1071059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1116059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1016059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1108059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1008059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1075059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1104059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1032059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1132059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1051059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/119059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1120059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1020059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1043059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1086059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1098059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1096059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1018059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1118059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/11059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1065059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1006059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1106059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1077059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1114059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1014059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1069059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/117059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1130059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1072059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1103059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1060059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1135059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1035059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1056059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1027059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1127059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1039059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1044059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1093059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1081059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1062059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1101059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1013059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1046059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1125059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1129059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1029059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1054059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1095059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1099059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1050059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1133059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1033059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1042059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/114059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1021059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1121059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1074059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1009059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1109059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1017059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1117059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1066059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1105059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/116059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1123059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1040059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1031059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1052059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1107059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1064059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1119059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1019059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1015059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1115059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1076059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1097059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/train/1089059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1180059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1157059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1145059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1173059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1161059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1147059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1136059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1155059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1171059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1190059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1179059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1194059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1198059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1184059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1188059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1196059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1165059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1141059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1153059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1160059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1148059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1156059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1139059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1193059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1183059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1191059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1162059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1158059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1154059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1195059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1150059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1142059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1174059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1166059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1178059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1152059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1164059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1168059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1176059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1185059.jpg\n","tagged-anime-illustrations-141-real-edge-pair/test/1197059.jpg\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2CbTEt448b4R"},"source":["BUFFER_SIZE = 100\n","BATCH_SIZE = 1\n","IMG_WIDTH = 256\n","IMG_HEIGHT = 256"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tyaP4hLJ8b4W"},"source":["def load_image(image_file, is_train):\n","  image = tf.read_file(image_file)\n","  image = tf.image.decode_jpeg(image)\n","\n","  w = tf.shape(image)[1]\n","\n","  w = w // 2\n","  real_image = image[:, :w, :]\n","  input_image = image[:, w:, :]\n","\n","  input_image = tf.cast(input_image, tf.float32)\n","  real_image = tf.cast(real_image, tf.float32)\n","\n","  if is_train:\n","    # random jittering\n","    \n","    # resizing to 286 x 286 x 3\n","    input_image = tf.image.resize_images(input_image, [286, 286], \n","                                        align_corners=True, \n","                                        method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","    real_image = tf.image.resize_images(real_image, [286, 286], \n","                                        align_corners=True, \n","                                        method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","    \n","    # randomly cropping to 256 x 256 x 3\n","    stacked_image = tf.stack([input_image, real_image], axis=0)\n","    cropped_image = tf.random_crop(stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n","    input_image, real_image = cropped_image[0], cropped_image[1]\n","\n","    if np.random.random() > 0.5:\n","      # random mirroring\n","      input_image = tf.image.flip_left_right(input_image)\n","      real_image = tf.image.flip_left_right(real_image)\n","  else:\n","    input_image = tf.image.resize_images(input_image, size=[IMG_HEIGHT, IMG_WIDTH], \n","                                         align_corners=True, method=2)\n","    real_image = tf.image.resize_images(real_image, size=[IMG_HEIGHT, IMG_WIDTH], \n","                                        align_corners=True, method=2)\n","  \n","  # normalizing the images to [-1, 1]\n","  input_image = (input_image / 127.5) - 1\n","  real_image = (real_image / 127.5) - 1\n","\n","  return input_image, real_image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PIGN6ouoQxt3"},"source":["## Use tf.data to create batches, map(do preprocessing) and shuffle the dataset"]},{"cell_type":"code","metadata":{"id":"SQHmYSmk8b4b"},"source":["train_dataset = tf.data.Dataset.list_files(PATH + 'train/*.jpg')\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n","train_dataset = train_dataset.map(lambda x: load_image(x, True))\n","train_dataset = train_dataset.batch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MS9J0yA58b4g"},"source":["test_dataset = tf.data.Dataset.list_files(PATH + '/test/*.jpg')\n","test_dataset = test_dataset.map(lambda x: load_image(x, False))\n","test_dataset = test_dataset.batch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"THY-sZMiQ4UV"},"source":["## Write the generator and discriminator models\n","\n","* **Generator** \n","  * The architecture of generator is a modified U-Net.\n","  * Each block in the encoder is (Conv -> Batchnorm -> Leaky ReLU)\n","  * Each block in the decoder is (Transposed Conv -> Batchnorm -> Dropout(applied to the first 3 blocks) -> ReLU)\n","  * There are skip connections between the encoder and decoder (as in U-Net).\n","  \n","* **Discriminator**\n","  * The Discriminator is a PatchGAN.\n","  * Each block in the discriminator is (Conv -> BatchNorm -> Leaky ReLU)\n","  * The shape of the output after the last layer is (batch_size, 30, 30, 1)\n","  * Each 30x30 patch of the output classifies a 70x70 portion of the input image (such an architecture is called a PatchGAN).\n","  * Discriminator receives 2 inputs.\n","    * Input image and the target image, which it should classify as real.\n","    * Input image and the generated image (output of generator), which it should classify as fake. \n","    * We concatenate these 2 inputs together in the code (`tf.concat([inp, tar], axis=-1)`)\n","\n","* Shape of the input travelling through the generator and the discriminator is in the comments in the code.\n","\n","To learn more about the architecture and the hyperparameters you can refer the [paper](https://arxiv.org/abs/1611.07004).\n","    "]},{"cell_type":"code","metadata":{"id":"tqqvWxlw8b4l"},"source":["OUTPUT_CHANNELS = 3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lFPI4Nu-8b4q"},"source":["class Downsample(tf.keras.Model):\n","    \n","  def __init__(self, filters, size, apply_batchnorm=True):\n","    super(Downsample, self).__init__()\n","    self.apply_batchnorm = apply_batchnorm\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","\n","    self.conv1 = tf.keras.layers.Conv2D(filters, \n","                                        (size, size), \n","                                        strides=2, \n","                                        padding='same',\n","                                        kernel_initializer=initializer,\n","                                        use_bias=False)\n","    if self.apply_batchnorm:\n","        self.batchnorm = tf.keras.layers.BatchNormalization()\n","  \n","  def call(self, x, training):\n","    x = self.conv1(x)\n","    if self.apply_batchnorm:\n","        x = self.batchnorm(x, training=training)\n","    x = tf.nn.leaky_relu(x)\n","    return x \n","\n","\n","class Upsample(tf.keras.Model):\n","    \n","  def __init__(self, filters, size, apply_dropout=False):\n","    super(Upsample, self).__init__()\n","    self.apply_dropout = apply_dropout\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","\n","    self.up_conv = tf.keras.layers.Conv2DTranspose(filters, \n","                                                   (size, size), \n","                                                   strides=2, \n","                                                   padding='same',\n","                                                   kernel_initializer=initializer,\n","                                                   use_bias=False)\n","    self.batchnorm = tf.keras.layers.BatchNormalization()\n","    if self.apply_dropout:\n","        self.dropout = tf.keras.layers.Dropout(0.5)\n","\n","  def call(self, x1, x2, training):\n","    x = self.up_conv(x1)\n","    x = self.batchnorm(x, training=training)\n","    if self.apply_dropout:\n","        x = self.dropout(x, training=training)\n","    x = tf.nn.relu(x)\n","    x = tf.concat([x, x2], axis=-1)\n","    return x\n","\n","\n","class Generator(tf.keras.Model):\n","    \n","  def __init__(self):\n","    super(Generator, self).__init__()\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    \n","    self.down1 = Downsample(64, 4, apply_batchnorm=False)\n","    self.down2 = Downsample(128, 4)\n","    self.down3 = Downsample(256, 4)\n","    self.down4 = Downsample(512, 4)\n","    self.down5 = Downsample(512, 4)\n","    self.down6 = Downsample(512, 4)\n","    self.down7 = Downsample(512, 4)\n","    self.down8 = Downsample(512, 4)\n","\n","    self.up1 = Upsample(512, 4, apply_dropout=True)\n","    self.up2 = Upsample(512, 4, apply_dropout=True)\n","    self.up3 = Upsample(512, 4, apply_dropout=True)\n","    self.up4 = Upsample(512, 4)\n","    self.up5 = Upsample(256, 4)\n","    self.up6 = Upsample(128, 4)\n","    self.up7 = Upsample(64, 4)\n","\n","    self.last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, \n","                                                (4, 4), \n","                                                strides=2, \n","                                                padding='same',\n","                                                kernel_initializer=initializer)\n","  \n","  @tf.contrib.eager.defun\n","  def call(self, x, training):\n","    # x shape == (bs, 256, 256, 3)    \n","    x1 = self.down1(x, training=training) # (bs, 128, 128, 64)\n","    x2 = self.down2(x1, training=training) # (bs, 64, 64, 128)\n","    x3 = self.down3(x2, training=training) # (bs, 32, 32, 256)\n","    x4 = self.down4(x3, training=training) # (bs, 16, 16, 512)\n","    x5 = self.down5(x4, training=training) # (bs, 8, 8, 512)\n","    x6 = self.down6(x5, training=training) # (bs, 4, 4, 512)\n","    x7 = self.down7(x6, training=training) # (bs, 2, 2, 512)\n","    x8 = self.down8(x7, training=training) # (bs, 1, 1, 512)\n","\n","    x9 = self.up1(x8, x7, training=training) # (bs, 2, 2, 1024)\n","    x10 = self.up2(x9, x6, training=training) # (bs, 4, 4, 1024)\n","    x11 = self.up3(x10, x5, training=training) # (bs, 8, 8, 1024)\n","    x12 = self.up4(x11, x4, training=training) # (bs, 16, 16, 1024)\n","    x13 = self.up5(x12, x3, training=training) # (bs, 32, 32, 512)\n","    x14 = self.up6(x13, x2, training=training) # (bs, 64, 64, 256)\n","    x15 = self.up7(x14, x1, training=training) # (bs, 128, 128, 128)\n","\n","    x16 = self.last(x15) # (bs, 256, 256, 3)\n","    x16 = tf.nn.tanh(x16)\n","\n","    return x16"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ll6aNeQx8b4v"},"source":["class DiscDownsample(tf.keras.Model):\n","    \n","  def __init__(self, filters, size, apply_batchnorm=True):\n","    super(DiscDownsample, self).__init__()\n","    self.apply_batchnorm = apply_batchnorm\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","\n","    self.conv1 = tf.keras.layers.Conv2D(filters, \n","                                        (size, size), \n","                                        strides=2, \n","                                        padding='same',\n","                                        kernel_initializer=initializer,\n","                                        use_bias=False)\n","    if self.apply_batchnorm:\n","        self.batchnorm = tf.keras.layers.BatchNormalization()\n","  \n","  def call(self, x, training):\n","    x = self.conv1(x)\n","    if self.apply_batchnorm:\n","        x = self.batchnorm(x, training=training)\n","    x = tf.nn.leaky_relu(x)\n","    return x \n","\n","class Discriminator(tf.keras.Model):\n","    \n","  def __init__(self):\n","    super(Discriminator, self).__init__()\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    \n","    self.down1 = DiscDownsample(64, 4, False)\n","    self.down2 = DiscDownsample(128, 4)\n","    self.down3 = DiscDownsample(256, 4)\n","    \n","    # we are zero padding here with 1 because we need our shape to \n","    # go from (batch_size, 32, 32, 256) to (batch_size, 31, 31, 512)\n","    self.zero_pad1 = tf.keras.layers.ZeroPadding2D()\n","    self.conv = tf.keras.layers.Conv2D(512, \n","                                       (4, 4), \n","                                       strides=1, \n","                                       kernel_initializer=initializer, \n","                                       use_bias=False)\n","    self.batchnorm1 = tf.keras.layers.BatchNormalization()\n","    \n","    # shape change from (batch_size, 31, 31, 512) to (batch_size, 30, 30, 1)\n","    self.zero_pad2 = tf.keras.layers.ZeroPadding2D()\n","    self.last = tf.keras.layers.Conv2D(1, \n","                                       (4, 4), \n","                                       strides=1,\n","                                       kernel_initializer=initializer)\n","  \n","  @tf.contrib.eager.defun\n","  def call(self, inp, tar, training):\n","    # concatenating the input and the target\n","    x = tf.concat([inp, tar], axis=-1) # (bs, 256, 256, channels*2)\n","    x = self.down1(x, training=training) # (bs, 128, 128, 64)\n","    x = self.down2(x, training=training) # (bs, 64, 64, 128)\n","    x = self.down3(x, training=training) # (bs, 32, 32, 256)\n","\n","    x = self.zero_pad1(x) # (bs, 34, 34, 256)\n","    x = self.conv(x)      # (bs, 31, 31, 512)\n","    x = self.batchnorm1(x, training=training)\n","    x = tf.nn.leaky_relu(x)\n","    \n","    x = self.zero_pad2(x) # (bs, 33, 33, 512)\n","    # don't add a sigmoid activation here since\n","    # the loss function expects raw logits.\n","    x = self.last(x)      # (bs, 30, 30, 1)\n","\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gDkA05NE6QMs"},"source":["# The call function of Generator and Discriminator have been decorated\n","# with tf.contrib.eager.defun()\n","# We get a performance speedup if defun is used (~25 seconds per epoch)\n","generator = Generator()\n","discriminator = Discriminator()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0FMYgY_mPfTi"},"source":["## Define the loss functions and the optimizer\n","\n","* **Discriminator loss**\n","  * The discriminator loss function takes 2 inputs; **real images, generated images**\n","  * real_loss is a sigmoid cross entropy loss of the **real images** and an **array of ones(since these are the real images)**\n","  * generated_loss is a sigmoid cross entropy loss of the **generated images** and an **array of zeros(since these are the fake images)**\n","  * Then the total_loss is the sum of real_loss and the generated_loss\n","  \n","* **Generator loss**\n","  * It is a sigmoid cross entropy loss of the generated images and an **array of ones**.\n","  * The [paper](https://arxiv.org/abs/1611.07004) also includes L1 loss which is MAE (mean absolute error) between the generated image and the target image.\n","  * This allows the generated image to become structurally similar to the target image.\n","  * The formula to calculate the total generator loss = gan_loss + LAMBDA * l1_loss, where LAMBDA = 100. This value was decided by the authors of the [paper](https://arxiv.org/abs/1611.07004)."]},{"cell_type":"code","metadata":{"id":"cyhxTuvJyIHV"},"source":["LAMBDA = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wkMNfBWlT-PV"},"source":["def discriminator_loss(disc_real_output, disc_generated_output):\n","  real_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels = tf.ones_like(disc_real_output), \n","                                              logits = disc_real_output)\n","  generated_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels = tf.zeros_like(disc_generated_output), \n","                                                   logits = disc_generated_output)\n","\n","  total_disc_loss = real_loss + generated_loss\n","\n","  return total_disc_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"90BIcCKcDMxz"},"source":["def generator_loss(disc_generated_output, gen_output, target):\n","  gan_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels = tf.ones_like(disc_generated_output),\n","                                             logits = disc_generated_output) \n","  # mean absolute error\n","  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n","\n","  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n","\n","  return total_gen_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iWCn_PVdEJZ7"},"source":["generator_optimizer = tf.train.AdamOptimizer(2e-4, beta1=0.5)\n","discriminator_optimizer = tf.train.AdamOptimizer(2e-4, beta1=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aKUZnDiqQrAh"},"source":["## Checkpoints (Object-based saving)"]},{"cell_type":"code","metadata":{"id":"WJnftd5sQsv6"},"source":["checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n","                                 discriminator_optimizer=discriminator_optimizer,\n","                                 generator=generator,\n","                                 discriminator=discriminator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rw1fkAczTQYh"},"source":["## Training\n","\n","* We start by iterating over the dataset\n","* The generator gets the input image and we get a generated output.\n","* The discriminator receives the input_image and the generated image as the first input. The second input is the input_image and the target_image.\n","* Next, we calculate the generator and the discriminator loss.\n","* Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.\n","\n","## Generate Images\n","\n","* After training, its time to generate some images!\n","* We pass images from the test dataset to the generator.\n","* The generator will then translate the input image into the output we expect.\n","* Last step is to plot the predictions and **voila!**"]},{"cell_type":"code","metadata":{"id":"NS2GWywBbAWo"},"source":["EPOCHS = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RmdVsmvhPxyy"},"source":["def generate_images(model, test_input, tar):\n","  # the training=True is intentional here since\n","  # we want the batch statistics while running the model\n","  # on the test dataset. If we use training=False, we will get \n","  # the accumulated statistics learned from the training dataset\n","  # (which we don't want)\n","  prediction = model(test_input, training=True)\n","  plt.figure(figsize=(15,15))\n","\n","  display_list = [test_input[0], tar[0], prediction[0]]\n","  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n","\n","  for i in range(3):\n","    plt.subplot(1, 3, i+1) \n","    plt.title(title[i])\n","    # getting the pixel values between [0, 1] to plot it.\n","    plt.imshow(display_list[i] * 0.5 + 0.5)\n","    # plt.imshow(display_list[2] * 0.5 + 0.5)\n","    plt.axis('off')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2M7LmLtGEMQJ"},"source":["def train(dataset, epochs):  \n","  for epoch in range(epochs):\n","    start = time.time()\n","\n","    for input_image, target in dataset:\n","\n","      with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","        gen_output = generator(input_image, training=True)\n","\n","        disc_real_output = discriminator(input_image, target, training=True)\n","        disc_generated_output = discriminator(input_image, gen_output, training=True)\n","\n","        gen_loss = generator_loss(disc_generated_output, gen_output, target)\n","        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n","\n","      generator_gradients = gen_tape.gradient(gen_loss, \n","                                              generator.variables)\n","      discriminator_gradients = disc_tape.gradient(disc_loss, \n","                                                   discriminator.variables)\n","\n","      generator_optimizer.apply_gradients(zip(generator_gradients, \n","                                              generator.variables))\n","      discriminator_optimizer.apply_gradients(zip(discriminator_gradients, \n","                                                  discriminator.variables))\n","\n","    if epoch % 1 == 0:\n","        #clear_output(wait=True)\n","        for inp, tar in test_dataset.take(1):\n","          generate_images(generator, inp, tar)\n","          \n","    # saving (checkpoint) the model every 20 epochs\n","    if (epoch + 1) % 20 == 0:\n","      checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                        time.time()-start))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a1zZmKmvOH85","outputId":"a85a3156-856b-440b-c5fa-e540213ed53c","executionInfo":{"status":"ok","timestamp":1547984641973,"user_tz":-540,"elapsed":28865034,"user":{"displayName":"j s","photoUrl":"","userId":"00392485866776949616"}},"colab":{"base_uri":"https://localhost:8080/","height":37309,"output_embedded_package_id":"11Hf5PPiiltxUXxdM7tT9tO2BuZaWDUQu"}},"source":["train(train_dataset, EPOCHS)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"kz80bY3aQ1VZ"},"source":["## Restore the latest checkpoint and test"]},{"cell_type":"code","metadata":{"id":"4t4x69adQ5xb","outputId":"bf62bca4-7970-49e7-818c-0a727e9bda61","executionInfo":{"status":"ok","timestamp":1547984644063,"user_tz":-540,"elapsed":2093,"user":{"displayName":"j s","photoUrl":"","userId":"00392485866776949616"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# restoring the latest checkpoint in checkpoint_dir\n","checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7ff4dc8565f8>"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"1RGysMU_BZhx"},"source":["## Testing on the entire test dataset"]},{"cell_type":"code","metadata":{"id":"KUgSnmy2nqSP","outputId":"31af5beb-25da-4af5-b04e-01faabec99c4","executionInfo":{"status":"ok","timestamp":1547984689528,"user_tz":-540,"elapsed":45467,"user":{"displayName":"j s","photoUrl":"","userId":"00392485866776949616"}},"colab":{"base_uri":"https://localhost:8080/","height":14063,"output_embedded_package_id":"1CJ321FAJBMO1JH9PgGPNcW8dwpQHgNlV"}},"source":["# Run the trained model on the entire test dataset\n","for inp, tar in test_dataset:\n","  generate_images(generator, inp, tar)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}